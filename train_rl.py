import torch
import numpy as np
from qkd_env import QKDEnv
from rl_models import PolicyGradientAgent
import config
import os
from tqdm import tqdm
import time

from utils.traffic_generater import gen_traffic_matrix

def train(bypass=True):
    # Hyperparameters
    num_episodes = 500 if bypass else 100 
    batch_size = 5 # æ¯ 5 ä¸ª episode æ›´æ–°ä¸€æ¬¡ (æ”¹åŠ¨ A)
    lr = 1e-4 
    map_name = "Paris"
    protocol = "BB84"
    detector = "SNSPD"
    traffic_mid = "Low" # Respecting string input
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    strategy = "Bypass" if bypass else "NoBypass"
    print(f"\nğŸš€ Starting RL training for {strategy}...")
    
    import config
    config.bypass = bypass
    config.protocol = protocol
    config.detector = detector
    
    # Respect original input: Wavelength list from main.py
    wavelength_list = np.linspace(1530, 1565, 10).tolist()
    
    # Generate original request list
    request_list = gen_traffic_matrix(traffic_mid, map_name, wavelength_list, protocol, detector)
    
    # Initialize Environment with original inputs
    env = QKDEnv(
        map_name=map_name, 
        protocol=protocol, 
        detector=detector, 
        traffic_mid=traffic_mid,
        wavelength_list=wavelength_list,
        request_list=request_list
    )
    print(f"Network nodes: {env.num_nodes}")
    
    # å¯¹é½å®é™…èŠ‚ç‚¹æ•°åˆå§‹åŒ– Agent
    # ä¸ºä¸åŒåœºæ™¯åˆ†é…ç‹¬ç«‹çš„æ¶æ„å’Œå­¦ä¹ ç‡
    lr_actual = lr if bypass else lr * 2.0 # NoBypass ç®€å•ï¼Œå¯ä»¥å¿«ä¸€ç‚¹
    agent = PolicyGradientAgent(num_nodes=env.num_nodes, lr=lr_actual, is_bypass=bypass, device=device)
    
    # Create log directory
    os.makedirs("logs", exist_ok=True)
    log_file = open(f"logs/rl_{strategy}.log", "w")
    
    best_power = float('inf')
    best_state_dict = None 
    last_loss = 0.0 # è®°å½•æœ€è¿‘ä¸€æ¬¡æ›´æ–°çš„ lossï¼Œé¿å…ç§¯æ”’æœŸæ˜¾ç¤º 0
    
    # æ¢ç´¢é€€ç«
    start_entropy = 0.01
    end_entropy = 0.001
    
    for episode in range(num_episodes):
        start_time = time.time()
        state_matrices, context = env.reset()
        agent.reset_episode() 
        done = False
        episode_reward = 0
        
        # çº¿æ€§é€€ç«è®¡ç®—å½“å‰çš„ entropy æƒé‡
        entropy_coef = max(end_entropy, start_entropy - (start_entropy - end_entropy) * (episode / num_episodes))
        
        while not done:
            action_weights = agent.select_action(state_matrices, context, train=True)
            next_state, reward, done, info = env.step(action_weights)
            episode_reward += reward
            state_matrices, context = next_state
            
        # ç»“æŸ Episodeï¼Œè®°å½•æ•°æ®
        agent.end_episode(episode_reward)
        
        # ç´¯è®¡åˆ° batch_size åæ‰§è¡Œæ›´æ–°
        if (episode + 1) % batch_size == 0:
            last_loss = agent.update(entropy_coef=entropy_coef)
            
        duration = time.time() - start_time
        avg_power = info.get('avg_power', 0)
        spec_occ = info.get('spec_occ', 0)
        
        status_str = f"[{strategy}] Ep {episode+1} | Reward: {episode_reward:.2f} | Power: {avg_power:.2f}W | Spec: {spec_occ:.4f} | Loss: {last_loss:.4f} | Ent: {entropy_coef:.4f} | {duration:.1f}s"
        print(status_str)
        log_file.write(status_str + "\n")
        log_file.flush()
        
        # ç²¾è‹±ç­–ç•¥ä¿å­˜ä¸å®šæœŸå›æ»š (æ”¹åŠ¨ C)
        if avg_power < best_power and avg_power > 0:
            best_power = avg_power
            best_state_dict = agent.model.state_dict().copy()
            os.makedirs("models", exist_ok=True)
            agent.save(f"models/qkd_rl_{strategy}_best.pth")
            
        # æ¯ 50 ä¸ª episodeï¼Œå¦‚æœå½“å‰æ€§èƒ½é€€åŒ–ä¸¥é‡ï¼Œè€ƒè™‘å›æ»šåˆ°ç²¾è‹±ç­–ç•¥
        if (episode + 1) % 50 == 0 and best_state_dict is not None:
            # è¿™é‡Œå¯ä»¥ç”¨ä¸€ä¸ªç®€å•çš„æ¦‚ç‡æˆ–è€…é˜ˆå€¼åˆ¤æ–­æ˜¯å¦å›æ»š
            # ä¸ºäº†ç¨³å®šï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨æ€§èƒ½æ²¡èƒ½çªç ´æ—¶ï¼Œå°æ¦‚ç‡(20%)å›æ»šä»¥é‡æ–°æ¢ç´¢
            if avg_power > best_power * 1.1 and np.random.random() < 0.2:
                print(f"ğŸ”„ Rolling back to best policy (Power: {best_power:.2f}W)")
                agent.model.load_state_dict(best_state_dict)

    log_file.close()
    return best_power

if __name__ == "__main__":
    print("Main script started")
    # è·³è¿‡ NoBypassï¼Œç›´æ¥æ”»å…‹æœ€å…·æŒ‘æˆ˜æ€§çš„ Bypass æ¨¡å¼
    # p_nobypass = train(bypass=False)
    p_bypass = train(bypass=True)
    
    print("\n" + "="*40)
    print(f"Final Best Power (Bypass): {p_bypass:.2f}W")
    # print(f"Final Best Power (NoBypass): {p_nobypass:.2f}W")
    print("="*40)
