import torch
import torch.nn as nn
import numpy as np
import cma
import time
import os
import json
import gc

# ç¯å¢ƒå˜é‡è®¾ç½®å¿…é¡»åœ¨å¯¼å…¥ numpy/torch ä¹‹åå°½å¿«æ‰§è¡Œï¼Œæˆ–åœ¨æœ€å‰é¢
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"

from qkd_env import QKDEnv
from rl_models import QKDGraphNet
from utils.traffic_generater import gen_traffic_matrix

class CMAESOptimizer:
    def __init__(self, bypass=True, map_name="Paris", traffic_mid="Low", protocol="BB84", detector="SNSPD", device="cuda"):
        self.bypass = bypass
        self.protocol = protocol
        self.detector = detector
        self.traffic_mid = traffic_mid
        self.device = device
        self.wavelength_list = np.linspace(1530, 1565, 10).tolist()
        
        # æ ¸å¿ƒä¿®å¤ï¼šè®¾ç½®å›ºå®šéšæœºç§å­ï¼Œç¡®ä¿æ¯æ¬¡ç”Ÿæˆçš„ request_list å®Œå…¨ä¸€è‡´
        # è¿™å¯¹äºè·¨è¿è¡Œçš„ Warm Start å’Œå…¬å¹³æ¯”è¾ƒè‡³å…³é‡è¦
        import random
        random.seed(42)
        np.random.seed(42)
        torch.manual_seed(42)
        
        self.request_list = gen_traffic_matrix(traffic_mid, map_name, self.wavelength_list, protocol, detector)
        print(f"âœ… Generated request list (Size: {len(self.request_list)}) with seed 42. Consistent across runs.")
        
        # åˆå§‹åŒ–ç¯å¢ƒ
        self.env = QKDEnv(
            map_name=map_name,
            protocol=protocol,
            detector=detector,
            traffic_mid=traffic_mid,
            wavelength_list=self.wavelength_list,
            request_list=self.request_list
        )
        
        # åˆå§‹åŒ– GNN æ¨¡å‹
        self.model = QKDGraphNet(actual_nodes=self.env.num_nodes, is_bypass=bypass, hidden_dim=8).to(device)
        self.param_shapes = [p.shape for p in self.model.parameters()]
        self.param_sizes = [p.numel() for p in self.model.parameters()]
        self.total_params = sum(self.param_sizes)
        
        # ç‹¬ç«‹æ¨¡å‹ä¿å­˜è·¯å¾„ (å¢åŠ  _GNN åç¼€åŒºåˆ†)
        self.model_filename = f"gnn_best_Paris_{protocol}_{detector}_{traffic_mid}_bypass_{bypass}.pth"
        
        print(f"ğŸš€ GNN-CMA-ES Optimizer Initialized. Total Parameters: {self.total_params}")

    def vector_to_model(self, vector):
        """å°†ä¸€ç»´å‘é‡è¿˜åŸå›æ¨¡å‹å‚æ•°"""
        state_dict = self.model.state_dict()
        curr_idx = 0
        for name, param in self.model.named_parameters():
            size = param.numel()
            new_param = torch.from_numpy(vector[curr_idx:curr_idx+size]).view(param.shape).float().to(self.device)
            param.data.copy_(new_param)
            curr_idx += size

    def evaluate(self, vector):
        """è¯„ä¼°ä¸€ä¸ªå‚æ•°å‘é‡çš„ Fitness (Total Avg Power + Occupied Spectrum)"""
        self.vector_to_model(vector)
        self.model.eval()
        
        state_matrices, context = self.env.reset()
        h_state = torch.zeros(1, 8).to(self.device)
        last_action_t = None
        done = False
        
        while not done:
            with torch.no_grad():
                state_t = torch.FloatTensor(state_matrices).unsqueeze(0).to(self.device)
                context_t = torch.FloatTensor(context).unsqueeze(0).to(self.device)
                mu, _, h_next = self.model(state_t, context_t, last_action_t, h_state)
                h_state = h_next
                action_weights = mu.squeeze().cpu().numpy()
                last_action_t = mu.view(1, -1)
                
            next_state, reward, done, info = self.env.step(action_weights)
            state_matrices, context = next_state
            
        # ä¼˜åŒ–ç›®æ ‡ï¼šTotal Avg Power + Occupied Spectrum
        # æ³¨æ„ï¼šOccupied Spectrum < 1ï¼Œä½œä¸ºå¹³æ»‘é¡¹
        avg_power = info.get('avg_power', 10000.0)
        spec_occ = info.get('spec_occ', 1.0)
        fitness = avg_power + spec_occ
        
        # è®°å½•è¯¦ç»†ä¿¡æ¯ï¼Œä¾›ä¿å­˜æ—¶ä½¿ç”¨
        self.last_info = info
        
        return fitness

    def save_callback(self, es):
        """CMA-ES æ¯ä¸€ä»£ç»“æŸåçš„å›è°ƒï¼Œç”¨äºä¿å­˜æ¨¡å‹å’Œå½“å‰æœ€ä¼˜è¯¦ç»†æ•°æ®"""
        if es.result.fbest < self.best_power_found:
            self.best_power_found = es.result.fbest
            # å°†æœ€ä¼˜å‘é‡è¿˜åŸåˆ°æ¨¡å‹å¹¶ä¿å­˜
            self.vector_to_model(es.result.xbest)
            model_path = os.path.join("models", self.model_filename)
            tmp_path = model_path + ".tmp"
            torch.save(self.model.state_dict(), tmp_path)
            os.replace(tmp_path, model_path)
            
            # ä¿å­˜å½“å‰æœ€ä¼˜çš„ç‰©ç†æŒ‡æ ‡
            self.best_metrics = self.last_info
            
            print(f"âœ¨ New Best Fitness: {self.best_power_found:.4f} (Power: {self.best_metrics['avg_power']:.2f}W) | Model Saved: {self.model_filename}", flush=True)

    def train(self, max_iter=100, pop_size=64):
        # å°è¯•ä»ç°æœ‰æœ€ä¼˜æ¨¡å‹åŠ è½½ï¼Œè¿›è¡Œâ€œçƒ­å¯åŠ¨â€
        model_path = os.path.join("models", self.model_filename)
        self.best_metrics = {}
        
        if os.path.exists(model_path):
            print(f"ğŸ“‚ Found existing best model: {self.model_filename}. Loading for warm start...", flush=True)
            try:
                self.model.load_state_dict(torch.load(model_path))
            except:
                print(f"âš ï¸ Failed to load {self.model_filename}, starting from scratch.")
        
        # åˆå§‹å‡å€¼ä¸ºå½“å‰æ¨¡å‹å‚æ•°
        initial_params = np.concatenate([p.data.cpu().numpy().flatten() for p in self.model.parameters()])
        
        self.best_power_found = float('inf')
        
        print(f"ğŸš€ Starting BIPOP-CMA-ES optimization...", flush=True)
        
        # ä½¿ç”¨ cma.fmin2 ç›´æ¥è°ƒç”¨ BIPOP-CMA-ES
        # bipop=True: å¼€å¯ BIPOP é‡å¯ç­–ç•¥
        # restarts=9: å…è®¸æœ€å¤š 9 æ¬¡é‡å¯ï¼ˆåŒ…å« IPOP å¢åŠ ç§ç¾¤å’Œå°ç§ç¾¤æ¢ç´¢ï¼‰
        opts = {
            'popsize': pop_size, 
            'maxiter': max_iter, 
            'verb_disp': 1,
            'tolfunhist': 0, 
            'tolfun': 1e-12
        }
        
        # åŒ…è£… evaluate å‡½æ•°ï¼Œç¡®ä¿å¥å£®æ€§
        self.eval_count = 0
        def objective(x):
            try:
                self.eval_count += 1
                fit = self.evaluate(x)
                
                # æ˜¾å¼è§¦å‘åƒåœ¾å›æ”¶ï¼Œé˜²æ­¢å†…å­˜ç´¯ç§¯å¯¼è‡´çš„æ½œåœ¨æ®µé”™è¯¯
                if self.eval_count % 32 == 0:
                    gc.collect()
                
                if self.eval_count % 8 == 0:
                    print(f"  Eval {self.eval_count} | Power: {fit:.2f}W", flush=True)
                return float(fit) if np.isfinite(fit) else 10000.0
            except Exception as e:
                print(f"âŒ Error in objective: {e}", flush=True)
                return 10000.0

        res = cma.fmin2(
            objective, 
            initial_params, 
            0.3, 
            opts,
            callback=self.save_callback,
            bipop=True,
            restarts=9
        )
        
        print(f"\nâœ… Optimization Finished. Best Power: {res[1]:.2f}W")
        return res[1]

if __name__ == "__main__":
    import argparse
    import config
    os.makedirs("models", exist_ok=True)
    
    parser = argparse.ArgumentParser(description="Run CMA-ES Optimization for a specific configuration")
    parser.add_argument("--bypass", type=str, default="True", help="Bypass mode (True/False)")
    parser.add_argument("--detector", type=str, default="SNSPD", help="Detector type (SNSPD/APD/ThorlabsPDB)")
    parser.add_argument("--traffic", type=str, default="Low", help="Traffic level (Low/Medium/High)")
    parser.add_argument("--protocol", type=str, default="BB84", help="Protocol (BB84/CV-QKD)")
    parser.add_argument("--max_iter", type=int, default=300, help="Max iterations")
    parser.add_argument("--pop_size", type=int, default=64, help="Population size")
    
    args = parser.parse_args()
    
    # å°†å‚æ•°è½¬æ¢ä¸º bool
    is_bypass = args.bypass.lower() == "true"
    
    # æ£€æµ‹ CUDA çŠ¶æ€
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device} | Config: {args.protocol}, {args.detector}, {args.traffic}, Bypass={is_bypass}")
    
    # å¼ºåˆ¶å¼€å¯æŒ‡å®šæ¨¡å¼è¿›è¡Œæ”»åš
    optimizer = CMAESOptimizer(
        bypass=is_bypass, 
        map_name="Paris", 
        traffic_mid=args.traffic, 
        protocol=args.protocol,
        detector=args.detector,
        device=device
    )

    optimizer.env.provided_request_list = optimizer.request_list
    
    best_p = optimizer.train(max_iter=args.max_iter, pop_size=args.pop_size)
    
    # è¿”å›åŒ…å«æ‰€æœ‰ç‰©ç†æŒ‡æ ‡çš„å­—å…¸
    result_data = {
        "protocol": args.protocol,
        "detector": args.detector,
        "traffic": args.traffic,
        "bypass": is_bypass,
        "best_fitness": best_p,
        "avg_power": optimizer.best_metrics.get('avg_power', 10000.0),
        "spec_occ": optimizer.best_metrics.get('spec_occ', 1.0),
        "source_p": optimizer.best_metrics.get('source_p', 0.0),
        "detector_p": optimizer.best_metrics.get('detector_p', 0.0),
        "other_p": optimizer.best_metrics.get('other_p', 0.0),
        "ice_box_p": optimizer.best_metrics.get('ice_box_p', 0.0)
    }
    
    # ä¿å­˜ç»“æœåˆ°ç‹¬ç«‹æ–‡ä»¶ï¼Œä¾›æ±‡æ€»è„šæœ¬è¯»å–
    result_filename = f"results_Paris_{args.protocol}_{args.detector}_{args.traffic}_Bypass_{is_bypass}.json"
    with open(result_filename, "w") as f:
        json.dump(result_data, f)
    
    print("\n" + "="*40)
    print(f"Optimization Done! Results saved to {result_filename}")
    print(f"Final Power: {result_data['avg_power']:.2f}W | Spectrum: {result_data['spec_occ']:.4f}")
    print("="*40)
