import os
# åœ¨å¯¼å…¥ä»»ä½•å…¶ä»–åº“ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTHONWARNINGS"] = "ignore"

import warnings
import time
# å¿½ç•¥ Gym åºŸå¼ƒè­¦å‘Šå’Œ Matplotlib ç¼ºå¤±è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning, module="cma")
warnings.filterwarnings("ignore", category=DeprecationWarning, module="gym")
# å¯¹äº gym çš„ç‰¹å®šåºŸå¼ƒæ¶ˆæ¯ï¼Œå¯èƒ½éœ€è¦æ›´é€šç”¨çš„è¿‡æ»¤
warnings.filterwarnings("ignore", message=".*Gym has been unmaintained.*")

import torch
import torch.nn as nn
import numpy as np
import cma
import time
import os
import json
import gc
import random
import multiprocessing
import traceback

# ç¯å¢ƒå˜é‡è®¾ç½®
os.environ["MKL_THREADING_LAYER"] = "GNU"
os.environ["OMP_NUM_THREADS"] = "1"
os.environ["OPENBLAS_NUM_THREADS"] = "1"

from qkd_env import QKDEnv
from rl_models import QKDGraphNet
from utils.traffic_generater import gen_traffic_matrix
from concurrent.futures import ProcessPoolExecutor

# === Worker è¿›ç¨‹å†…çš„å…¨å±€å˜é‡ (è¿›ç¨‹éš”ç¦») ===
_WORKER_ENV = None
_WORKER_MODEL = None

def worker_initializer(map_name, protocol, detector, traffic_mid, wavelength_list, request_list, hidden_dim):
    """
    Worker è¿›ç¨‹åˆå§‹åŒ–å‡½æ•°ã€‚åªåœ¨è¿›ç¨‹å¯åŠ¨æ—¶æ‰§è¡Œä¸€æ¬¡ã€‚
    """
    try:
        # [Performance] å¼ºåˆ¶é™åˆ¶æ¯ä¸ª Worker çš„çº¿ç¨‹æ•°ï¼Œé˜²æ­¢ CPU è¿‡è½½
        import os
        os.environ["OMP_NUM_THREADS"] = "1"
        os.environ["MKL_NUM_THREADS"] = "1"
        os.environ["OPENBLAS_NUM_THREADS"] = "1"
        
        import torch
        torch.set_num_threads(1)
        torch.set_num_interop_threads(1)

        global _WORKER_ENV, _WORKER_MODEL
        
        # 1. ç¦ç”¨è­¦å‘Š
        import warnings
        warnings.filterwarnings("ignore")
        os.environ["PYTHONWARNINGS"] = "ignore"
        
        # 2. åˆå§‹åŒ–ç¯å¢ƒ (é»˜è®¤ is_bypass=Falseï¼Œä¼šåœ¨æ¯æ¬¡ evaluate æ—¶åŠ¨æ€ä¿®æ”¹)
        _WORKER_ENV = QKDEnv(
            map_name=map_name,
            protocol=protocol,
            detector=detector,
            traffic_mid=traffic_mid,
            wavelength_list=wavelength_list,
            request_list=request_list,
            is_bypass=False 
        )
        
        # 3. åˆå§‹åŒ–æ¨¡å‹ (CPU)
        _WORKER_MODEL = QKDGraphNet(actual_nodes=_WORKER_ENV.num_nodes, is_bypass=False, hidden_dim=hidden_dim).to("cpu")
        # è®¾ç½®ä¸º eval æ¨¡å¼ï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦æ¢¯åº¦
        _WORKER_MODEL.eval()
        
        # [Debug] ç¡®è®¤ Worker å¯åŠ¨æˆåŠŸ
        # with open(f"logs/worker_{os.getpid()}_ok.txt", "w") as f: f.write("OK")
        
    except Exception as e:
        import traceback
        import os
        os.makedirs("logs", exist_ok=True)
        with open(f"logs/worker_{os.getpid()}_error.txt", "w") as f:
            f.write(traceback.format_exc())
        raise e

# === ç‹¬ç«‹çš„ Worker å‡½æ•° ===
def evaluate_worker(args):
    """
    å¹¶è¡Œè¯„ä¼° Workerã€‚ç›´æ¥å¤ç”¨å…¨å±€å˜é‡ã€‚
    Args:
        args: tuple (vector, is_bypass)
    """
    global _WORKER_ENV, _WORKER_MODEL
    
    # [Robustness] å¢åŠ æ›´å…¨é¢çš„å¼‚å¸¸æ•è·ï¼Œé˜²æ­¢ Worker å´©æºƒ
    try:
        vector, is_bypass = args
        
        # 1. åŠ¨æ€æ›´æ–°é…ç½® (é˜²æ­¢çŠ¶æ€æ±¡æŸ“)
        _WORKER_ENV.is_bypass = is_bypass
        _WORKER_MODEL.is_bypass = is_bypass # å¦‚æœæ¨¡å‹å†…éƒ¨ç”¨åˆ°äº†è¿™ä¸ªæ ‡å¿—
        
        # 2. åŠ è½½å‚æ•°
        # vector æ˜¯ numpy arrayï¼Œè½¬ä¸º tensor
        curr_idx = 0
        for param in _WORKER_MODEL.parameters():
            size = param.numel()
            # è¿™ç§å†™æ³•æ¯” named_parameters æ›´å¿«ï¼Œä¸”æ— éœ€ name åŒ¹é…
            new_param = torch.from_numpy(vector[curr_idx:curr_idx+size]).view(param.shape).float()
            param.data.copy_(new_param)
            curr_idx += size
            
        # 3. è¿è¡Œè¯„ä¼°å¾ªç¯
        # reset ä¼šæ ¹æ® _WORKER_ENV.is_bypass å¼ºåˆ¶æ›´æ–° config.bypass
        state_matrices, context = _WORKER_ENV.reset()
        
        # è¿™é‡Œçš„ hidden_dim å¯ä»¥ä»æ¨¡å‹é‡Œå–
        hidden_dim = _WORKER_MODEL.hidden_dim if hasattr(_WORKER_MODEL, 'hidden_dim') else 8
        h_state = torch.zeros(1, hidden_dim)
        last_action_t = None
        done = False
        
        step_count = 0
        while not done:
            with torch.no_grad():
                x_global_np, x_wl_np = state_matrices
                # ä½¿ç”¨ from_numpy é¿å…å†…å­˜æ‹·è´ (Zero-copy)
                x_global_t = torch.from_numpy(x_global_np).float().unsqueeze(0)
                x_wl_t = torch.from_numpy(x_wl_np).float().unsqueeze(0)
                context_t = torch.from_numpy(context).float().unsqueeze(0)
                
                mu, _, h_next = _WORKER_MODEL(x_global_t, x_wl_t, context_t, last_action_t, h_state)
                h_state = h_next
                action_weights = mu.squeeze().numpy()
                last_action_t = mu.view(1, -1)
                
            next_state, reward, done, info = _WORKER_ENV.step(action_weights)
            state_matrices, context = next_state
            step_count += 1
            
        avg_power = info.get('avg_power', 10000.0)
        spec_occ = info.get('spec_occ', 1.0)
        
        # [New] Add detailed component power dict for analysis
        info['component_power'] = _WORKER_ENV.total_component_power
        
        # ç»¼åˆé€‚åº”åº¦ï¼šå¹³å‡åŠŸè€— + é¢‘è°±å ç”¨ + çƒ­èƒ½é£é™© + å¾®è§‚è·¯å¾„æƒé‡æƒ©ç½š
        path_cost_sum = info.get('path_cost', 0.0)
        fitness = avg_power + spec_occ
        # fitness = avg_power + spec_occ + 0.00001 * path_cost_sum
        
        return fitness, info
        
    except Exception as e:
        import traceback
        import sys
        # æ‰“å°å®Œæ•´çš„ traceback åˆ° stderrï¼Œç¡®ä¿èƒ½è¢«ä¸»è¿›ç¨‹æ•è·
        print(f"âŒ Worker Critical Error: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        return 10000.0, {}
    except BaseException as e: # æ•è· KeyboardInterrupt, SystemExit ç­‰æ›´åº•å±‚çš„å¼‚å¸¸
        import traceback
        import sys
        print(f"âŒ Worker Fatal Crash: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        return 10000.0, {}

class OpenAIESOptimizer:
    """
    OpenAI Evolution Strategies (ES) Optimizer
    ä½¿ç”¨ Adam ä¼˜åŒ–å™¨ + ä¼ªæ¢¯åº¦ä¼°è®¡ (Search Gradients) + Rank Shapingã€‚
    """
    def __init__(self, request_list, executor, bypass=True, map_name="Paris", traffic_mid="Low", protocol="BB84", detector="SNSPD", device="cuda", pop_size=64):
        self.bypass = bypass
        self.map_name = map_name
        self.protocol = protocol
        self.detector = detector
        self.traffic_mid = traffic_mid
        self.device = device
        self.wavelength_list = np.linspace(1530, 1565, 10).tolist()
        self.hidden_dim = 8
        self.executor = executor
        self.request_list = request_list
        
        # 1. åˆå§‹åŒ–æ¨¡å‹
        self.env = QKDEnv(
            map_name=map_name, protocol=protocol, detector=detector, traffic_mid=traffic_mid,
            wavelength_list=self.wavelength_list, request_list=self.request_list, is_bypass=bypass
        )
        self.model = QKDGraphNet(
            num_global_features=7, num_wl_features=5, num_wavelengths=len(self.wavelength_list),
            actual_nodes=self.env.num_nodes, is_bypass=bypass, hidden_dim=self.hidden_dim
        ).to(device)
        
        self.total_params = sum(p.numel() for p in self.model.parameters())
        self.model_filename = f"gnn_best_{map_name}_{protocol}_{detector}_{traffic_mid}_bypass_{bypass}.pth"
        
        print(f"ğŸš€ OpenAI-ES Optimizer Initialized. Params: {self.total_params}")
        
        # 2. ES å‚æ•°
        self.pop_size = 128 # [Tuning] å¢å¤§ç§ç¾¤ä»¥å¢å¼ºæ¢ç´¢
        self.sigma = 0.15   # [Tuning] å¢å¤§åˆå§‹å™ªå£° (0.1 -> 0.15)
        self.lr = 0.02      # å­¦ä¹ ç‡ (Adam)
        # [Optimization] å‡å° Weight Decay (0.005 -> 0.001)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.001)
        
        # çŠ¶æ€è®°å½•
        self.best_fitness_found = float('inf')
        self.best_pure_power_found = float('inf')
        self.prev_best_fitness = float('inf') # [Adaptive Sigma] è®°å½•ä¸Šä¸€ä»£çš„æœ€ä½³
        self.best_metrics = {}
        self.generation = 0
        self.current_center_params = None
        self.stagnation_counter = 0 # [Restart] åœæ»è®¡æ•°å™¨
        
        # å°è¯•çƒ­å¯åŠ¨
        model_path = os.path.join("models", self.model_filename)
        if os.path.exists(model_path):
            try:
                self.model.load_state_dict(torch.load(model_path))
                print(f"ğŸ“‚ [{bypass}] Loaded warm start model: {self.model_filename}")
            except: pass
            
        # æ—¥å¿—
        self.log_filename = f"log_{map_name}_{protocol}_{detector}_{traffic_mid}_Bypass_{bypass}.txt"
        with open(self.log_filename, "w") as f:
            f.write(f"--- OpenAI-ES Training Log for Bypass={bypass} ---\n")
        self.log_file = open(self.log_filename, "a")

    def get_flat_params(self):
        return np.concatenate([p.data.cpu().numpy().flatten() for p in self.model.parameters()])

    def set_flat_params(self, flat_params):
        curr_idx = 0
        for param in self.model.parameters():
            size = param.numel()
            new_param = torch.from_numpy(flat_params[curr_idx:curr_idx+size]).view(param.shape).float().to(self.device)
            param.data.copy_(new_param)
            curr_idx += size

    def step(self):
        """æ‰§è¡Œä¸€ä»£ OpenAI-ES è®­ç»ƒ"""
        center_params = self.get_flat_params()
        self.current_center_params = center_params
        
        # 1. ç”Ÿæˆå™ªå£° (Antithetic Sampling)
        half_pop = self.pop_size // 2
        noise = np.random.randn(half_pop, self.total_params)
        
        # 2. å‡†å¤‡è¯„ä¼°å‚æ•°
        eval_params = []
        for i in range(half_pop):
            eval_params.append(center_params + self.sigma * noise[i])
            eval_params.append(center_params - self.sigma * noise[i])
            
        # 3. å¹¶è¡Œè¯„ä¼°
        args_list = [(x, self.bypass) for x in eval_params]
        start_time = time.time()
        
        fitnesses = []
        infos = []
        try:
            results = list(self.executor.map(evaluate_worker, args_list))
            for fit, info in results:
                fitnesses.append(fit)
                infos.append(info)
        except Exception as e:
            print(f"âŒ Parallel Error: {e}")
            return False
            
        duration = time.time() - start_time
        fitnesses = np.array(fitnesses)
        
        # 4. è®°å½•æœ€ä½³ç»“æœ
        min_idx = np.argmin(fitnesses)
        if fitnesses[min_idx] < self.best_fitness_found:
            self.best_fitness_found = fitnesses[min_idx]
            self.best_pure_power_found = infos[min_idx].get('avg_power', float('inf'))
            self.best_metrics = infos[min_idx]
            self.save_model(eval_params[min_idx])
            
        # 5. æ¢¯åº¦ä¼°è®¡ (Rank Transformation)
        ranks = np.zeros_like(fitnesses)
        ranks[fitnesses.argsort()] = np.arange(len(fitnesses))
        # Utility: Rank 0 (Best) -> 0.5, Rank N-1 (Worst) -> -0.5
        utilities = (len(fitnesses) - 1 - ranks) / (len(fitnesses) - 1) - 0.5
        utilities = (utilities - utilities.mean()) / (utilities.std() + 1e-8)
        
        grad = np.zeros(self.total_params)
        for i in range(half_pop):
            u_pos = utilities[2*i]
            u_neg = utilities[2*i+1]
            grad += noise[i] * (u_pos - u_neg)
            
        grad /= (half_pop * self.sigma)
        
        # 6. Adam æ›´æ–°
        self.set_flat_params(self.current_center_params)
        self.optimizer.zero_grad(set_to_none=True)
        
        curr_idx = 0
        for param in self.model.parameters():
            size = param.numel()
            g = torch.from_numpy(grad[curr_idx:curr_idx+size]).view(param.shape).float().to(self.device)
            param.grad = -g # Adam Descent towards better utility
            curr_idx += size
            
        self.optimizer.step()
        
        # 7. æ—¥å¿—
        if self.generation % 1 == 0:
            avg_power = infos[min_idx].get('avg_power', 0.0)
            spec_occ = infos[min_idx].get('spec_occ', 0.0)
            fit_std = np.std(fitnesses) # æ›¿æ¢ Unique Count ä¸º Fitness Std
            
            log_str = (f"Gen {self.generation} (ES) | Pop: {self.pop_size} | Sigma: {self.sigma:.3f} | Time: {duration:.2f}s | "
                       f"Cur: {avg_power:.2f}W (S:{spec_occ:.2%}) | Std: {fit_std:.2f} | Best: {self.best_pure_power_found:.2f}W")
            print(f"[{'Bypass' if self.bypass else 'NoBypass'}] {log_str}")
            #self.log_file.write(log_str + "\n")
            #self.log_file.flush()
            
        # 8. [Adaptive Sigma] åŠ¨æ€è°ƒæ•´å™ªå£°å¹…åº¦
        current_best_fit = fitnesses[min_idx]
        
        if self.prev_best_fitness == float('inf'):
            # ç¬¬ä¸€ä»£ï¼Œè·³è¿‡è°ƒæ•´ï¼Œä¿æŒåˆå§‹ Sigma
            pass
        elif current_best_fit < self.prev_best_fitness:
            # è¿›æ­¥äº† -> æ ¹æ®è¿›æ­¥å¹…åº¦è‡ªé€‚åº”æ”¶æ•›
            # è¿›æ­¥è¶Šå¤§ï¼Œæ”¶æ•›è¶Šå¿« (Sigma *= ratio)
            ratio = current_best_fit / (self.prev_best_fitness + 1e-8)
            # ä¿æŠ¤æ€§ Clipï¼Œé˜²æ­¢ ratio å¼‚å¸¸
            ratio = np.clip(ratio, 0.5, 0.99) 
            self.sigma *= ratio
        else:
            # åœæ» -> è†¨èƒ€
            self.sigma *= 1.02
            
        # [Sigma Clip] é™åˆ¶æœ€å¤§å™ªå£°å¹…åº¦ (0.10 - 0.25)
        self.sigma = np.clip(self.sigma, 0.1, 0.25)
        self.prev_best_fitness = self.best_fitness_found # æ›´æ–°å†å²æœ€ä½³åŸºå‡†
        
        # [Restart Mechanism] å¦‚æœ Sigma é•¿æœŸé¡¶åœ¨ä¸Šé™ (0.25)ï¼Œè¯´æ˜é™·å…¥æ·±å‘ï¼Œå¼ºåˆ¶é‡å¯
        if self.sigma >= 0.248:
            self.stagnation_counter += 1
        else:
            self.stagnation_counter = 0
            
        if self.stagnation_counter >= 10:
            print(f"âš ï¸ [{self.generation}] Stagnation detected! Restarting with large perturbation...")
            
            # 1. å‚æ•°å¤§è·³è·ƒ (Jump)
            current_params = self.get_flat_params()
            # æ‰°åŠ¨å¹…åº¦ 0.5 (å¯¹äºæƒé‡æ¥è¯´å·²ç»å¾ˆå¤§äº†)
            perturbation = np.random.randn(self.total_params) * 0.5
            self.set_flat_params(current_params + perturbation)
            
            # 2. é‡ç½®çŠ¶æ€
            self.sigma = 0.15
            self.stagnation_counter = 0
            # é‡ç½® Adam åŠ¨é‡ (ä¿ç•™ Weight Decay è®¾ç½®)
            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.001)
            
        self.generation += 1
        return True

    def save_model(self, best_params):
        self.set_flat_params(best_params)
        torch.save(self.model.state_dict(), f"models/{self.model_filename}")
        self.set_flat_params(self.current_center_params) 
        
    def get_best_result(self):
        return self.best_metrics
    
    def load_from_optimizer(self, other_opt):
        print(f"ğŸ”„ [{self.bypass}] Transferring weights...")
        try:
            fname = os.path.join("models", other_opt.model_filename)
            if os.path.exists(fname):
                self.model.load_state_dict(torch.load(fname))
            else:
                pass
        except: pass
        
        # é‡ç½® Adam (å¸¦ Weight Decay)
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr, weight_decay=0.005)
        print(f"âœ… Transfer complete. Adam reset.")

    def close(self):
        if self.log_file: self.log_file.close()

class CMAESOptimizer:
    def __init__(self, request_list, executor, bypass=True, map_name="Paris", traffic_mid="Low", protocol="BB84", detector="SNSPD", device="cuda", pop_size=64):
        self.bypass = bypass
        self.map_name = map_name
        self.protocol = protocol
        self.detector = detector
        self.traffic_mid = traffic_mid
        self.device = device
        self.wavelength_list = np.linspace(1530, 1565, 10).tolist()
        self.base_pop_size = pop_size # è®°å½•åŸºç¡€ç§ç¾¤å¤§å°
        self.hidden_dim = 8
        self.executor = executor
        
        random.seed(42)
        np.random.seed(42)
        torch.manual_seed(42)
        
        self.request_list = request_list
        print(f"âœ… [{bypass}] Initialized with external request list (Size: {len(self.request_list)})")
        
        self.env = QKDEnv(
            map_name=map_name,
            protocol=protocol,
            detector=detector,
            traffic_mid=traffic_mid,
            wavelength_list=self.wavelength_list,
            request_list=self.request_list,
            is_bypass=bypass
        )
        
        self.model = QKDGraphNet(actual_nodes=self.env.num_nodes, is_bypass=bypass, hidden_dim=self.hidden_dim).to(device)
        self.total_params = sum(p.numel() for p in self.model.parameters())
        self.model_filename = f"gnn_best_{map_name}_{protocol}_{detector}_{traffic_mid}_bypass_{bypass}.pth"
        
        print(f"ğŸš€ GNN-CMA-ES Optimizer Initialized. Total Parameters: {self.total_params}")
        print(f"   Hidden Dim: {self.hidden_dim} | GRU: Enabled | Structure: Spatial-Preserved GNN")
        
        # === BIPOP çŠ¶æ€è¿½è¸ª ===
        self.restart_count = 0
        self.last_large_pop = pop_size
        self.bipop_mode = "large" # å½“å‰æ¨¡å¼: 'large' (IPOP) æˆ– 'small' (Local)
        
        # åˆå§‹åŒ– CMA-ES å‚æ•°
        initial_params = np.concatenate([p.data.cpu().numpy().flatten() for p in self.model.parameters()])
        self.opts = {
            'popsize': pop_size, 
            'verb_disp': 0,
            'verb_log': 0,
            'tolfun': 1e-12
        }
        self.es = cma.CMAEvolutionStrategy(initial_params, 0.5, self.opts)
        
        # çŠ¶æ€è®°å½•
        self.best_fitness_found = float('inf')
        self.best_pure_power_found = float('inf')
        self.best_metrics = {}
        self.eval_count = 0
        self.generation = 0
        self.best_solution_vector = initial_params # è®°å½•å…¨å±€æœ€ä½³å‚æ•°ç”¨äºé‡å¯
        
        self.log_filename = f"log_{map_name}_{protocol}_{detector}_{traffic_mid}_Bypass_{bypass}.txt"
        with open(self.log_filename, "w") as f:
            f.write(f"--- BIPOP-CMA-ES Training Log for Bypass={bypass} ---\n")
        self.log_file = open(self.log_filename, "a")
        
        # å°è¯•çƒ­å¯åŠ¨
        model_path = os.path.join("models", self.model_filename)
        if os.path.exists(model_path):
            try:
                self.model.load_state_dict(torch.load(model_path))
                print(f"ğŸ“‚ [{bypass}] Loaded warm start model: {self.model_filename}")
                new_params = np.concatenate([p.data.cpu().numpy().flatten() for p in self.model.parameters()])
                self.es = cma.CMAEvolutionStrategy(new_params, 0.5, self.opts)
                self.best_solution_vector = new_params
            except:
                pass

    def vector_to_model(self, vector):
        state_dict = self.model.state_dict()
        curr_idx = 0
        for name, param in self.model.named_parameters():
            size = param.numel()
            new_param = torch.from_numpy(vector[curr_idx:curr_idx+size]).view(param.shape).float().to(self.device)
            param.data.copy_(new_param)
            curr_idx += size
            
    # evaluate å‡½æ•°ä¿æŒä¸å˜ ...
    def evaluate(self, vector):
        self.vector_to_model(vector)
        self.model.eval()
        
        state_matrices, context = self.env.reset()
        h_state = torch.zeros(1, 8).to(self.device)
        last_action_t = None
        done = False
        
        while not done:
            with torch.no_grad():
                x_global_np, x_wl_np = state_matrices
                x_global_t = torch.FloatTensor(x_global_np).unsqueeze(0).to(self.device)
                x_wl_t = torch.FloatTensor(x_wl_np).unsqueeze(0).to(self.device)
                context_t = torch.FloatTensor(context).unsqueeze(0).to(self.device)
                
                mu, _, h_next = self.model(x_global_t, x_wl_t, context_t, last_action_t, h_state)
                h_state = h_next
                action_weights = mu.squeeze().cpu().numpy()
                last_action_t = mu.view(1, -1)
                
            next_state, reward, done, info = self.env.step(action_weights)
            state_matrices, context = next_state
            
        avg_power = info.get('avg_power', 10000.0)
        spec_occ = info.get('spec_occ', 1.0)
        fitness = avg_power + spec_occ
        self.last_info = info
        return fitness

    def step(self):
        """æ‰§è¡Œä¸€ä»£è®­ç»ƒ (æ”¯æŒ BIPOP é‡å¯)"""
        
        # === BIPOP é‡å¯é€»è¾‘ ===
        if self.es.stop():
            self.restart_count += 1
            stop_reason = self.es.stop()
            print(f"ğŸ›‘ [{self.bypass}] Convergence detected: {stop_reason}. Triggering BIPOP Restart #{self.restart_count}")
            
            # åˆ‡æ¢ç­–ç•¥ï¼šå¦‚æœä¸Šæ¬¡æ˜¯å°ç§ç¾¤(Local)ï¼Œè¿™æ¬¡å°±å¤§ç§ç¾¤(Global)ï¼Œåä¹‹äº¦ç„¶
            # æ³¨æ„ï¼šåˆå§‹æ˜¯ largeï¼Œç¬¬ä¸€æ¬¡é‡å¯é€šå¸¸å°è¯• small 
            if self.bipop_mode == "large":
                # åˆ‡æ¢åˆ°å°ç§ç¾¤æ¨¡å¼ (Local Search)
                self.bipop_mode = "small"
                new_popsize = self.base_pop_size
                new_sigma = 0.2 # å°æ­¥é•¿ç²¾ç»†æœç´¢
                print(f"ğŸ”„ Restart Mode: SMALL (Local Search) | Pop: {new_popsize} | Sigma: {new_sigma}")
            else:
                # åˆ‡æ¢åˆ°å¤§ç§ç¾¤æ¨¡å¼ (Global Search IPOP)
                self.bipop_mode = "large"
                self.last_large_pop *= 2 # ç§ç¾¤ç¿»å€
                new_popsize = self.last_large_pop
                new_sigma = 0.5 # å¤§æ­¥é•¿
                print(f"ğŸ”„ Restart Mode: LARGE (Global Search) | Pop: {new_popsize} | Sigma: {new_sigma}")
            
            # ä½¿ç”¨å†å²æœ€ä½³è§£ä½œä¸ºæ–°èµ·ç‚¹
            best_param_mean = self.best_solution_vector
            
            # æ›´æ–°é…ç½®
            new_opts = self.opts.copy()
            new_opts['popsize'] = new_popsize
            new_opts['seed'] = np.random.randint(100000)
            
            # é‡å¯ ES å®ä¾‹
            self.es = cma.CMAEvolutionStrategy(best_param_mean, new_sigma, new_opts)
        
        # === æ­£å¸¸çš„ ask/tell æµç¨‹ ===
        solutions = self.es.ask()
        
        args_list = [(x, self.bypass) for x in solutions]
        
        fitnesses = []
        infos = []
        
        start_time = time.time()
        
        try:
            results = list(self.executor.map(evaluate_worker, args_list))
            for fit, info in results:
                fitnesses.append(fit)
                infos.append(info)
            duration = time.time() - start_time
                
            best_idx = np.argmin(fitnesses)
            self.last_info = infos[best_idx]
            
            # æ›´æ–°å…¨å±€æœ€ä½³è§£ (ç”¨äºé‡å¯)
            current_best_fit = fitnesses[best_idx]
            if current_best_fit < self.best_fitness_found:
                self.best_fitness_found = current_best_fit
                self.best_solution_vector = solutions[best_idx] # ä¿å­˜æœ€ä½³å‚æ•°å‘é‡
                self.save_model_parallel(solutions[best_idx], self.last_info)
            
            # æ—¥å¿—
            if self.generation % 1 == 0:
                info = self.last_info
                avg_power = info.get('avg_power', 0.0)
                spec_occ = info.get('spec_occ', 0.0)
                hist_best_p = self.best_pure_power_found if self.best_pure_power_found != float('inf') else 0.0
                
                log_str = (f"Gen {self.generation} (R{self.restart_count}-{self.bipop_mode[0].upper()}) | "
                           f"Pop: {self.es.popsize} | Time: {duration:.2f}s | "
                           f"Cur: {avg_power:.2f}W (Spec:{spec_occ:.2%}) | HistBest: {hist_best_p:.2f}W")
                print(f"[{'Bypass' if self.bypass else 'NoBypass'}] {log_str}")
                self.log_file.write(log_str + "\n")
                self.log_file.flush()
                
        except Exception as e:
            print(f"âŒ Parallel Execution Error: {e}")
            return False
        
        self.es.tell(solutions, fitnesses)
        self.generation += 1
        
        return True

    def save_model_parallel(self, xbest, info):
        self.best_metrics = info
        self.best_pure_power_found = self.best_metrics.get('avg_power', float('inf'))
        self.vector_to_model(xbest)
        model_path = os.path.join("models", self.model_filename)
        tmp_path = model_path + ".tmp"
        torch.save(self.model.state_dict(), tmp_path)
        os.replace(tmp_path, model_path)
        print(f"âœ¨ [{self.bypass}] New Best: {self.best_pure_power_found:.2f}W")

    def get_best_result(self):
        return {
            "best_fitness": self.best_fitness_found,
            "avg_power": self.best_pure_power_found,
            "spec_occ": self.best_metrics.get('spec_occ', 0.0),
            "components": {
                "source": self.best_metrics.get('source_p', 0.0),
                "detector": self.best_metrics.get('detector_p', 0.0),
                "ice_box": self.best_metrics.get('ice_box_p', 0.0),
                "other": self.best_metrics.get('other_p', 0.0)
            }
        }
        
    def load_from_optimizer(self, other_opt):
        print(f"ğŸ”„ [{self.bypass}] Transferring weights from {other_opt.bypass}...")
        best_model_path = os.path.join("models", other_opt.model_filename)
        if os.path.exists(best_model_path):
            self.model.load_state_dict(torch.load(best_model_path))
        else:
            self.model.load_state_dict(other_opt.model.state_dict())
        
        new_params = np.concatenate([p.data.cpu().numpy().flatten() for p in self.model.parameters()])
        # é‡ç½®é‡å¯çŠ¶æ€
        self.restart_count = 0
        self.last_large_pop = self.base_pop_size
        self.bipop_mode = "large"
        self.best_solution_vector = new_params
        
        self.es = cma.CMAEvolutionStrategy(new_params, 0.5, self.opts)
        print(f"âœ… Transfer complete. CMA-ES reset.")

    def close(self):
        if self.log_file:
            self.log_file.close()

def run_experiment(map_name, protocol, detector, traffic_mid):
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    
    print(f"ğŸ”¥ Starting Experiment: {map_name} | {protocol} | {detector} | {traffic_mid} on {device}")
    
    # ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å…å†²çª
    json_filename = f"results/temp_{map_name}_{protocol}_{detector}_{traffic_mid}.json"
    os.makedirs("results", exist_ok=True)
    
    import glob
    for f in glob.glob(f"models/gnn_best_{map_name}_{protocol}_{detector}_{traffic_mid}_*.pth"):
        try: os.remove(f) 
        except: pass
            
    random.seed(42)
    np.random.seed(42)
    wavelength_list = np.linspace(1530, 1565, 10).tolist()
    global_request_list = gen_traffic_matrix(traffic_mid, map_name, wavelength_list, protocol, detector)

    # increase_traffic_value = 20000000
    # for request_id in range(len(global_request_list)):
    #     old_tuple = global_request_list[request_id]
    #     new_tuple = old_tuple[:-1] + (old_tuple[-1] + increase_traffic_value,)
    #     global_request_list[request_id] = new_tuple

    print(f"ğŸŒ Generated Global Request List (Size: {len(global_request_list)})")
    
    hidden_dim = 8 # å…¨å±€é…ç½®æ”¹å› 8
    initargs = (map_name, protocol, detector, traffic_mid, wavelength_list, global_request_list, hidden_dim)
    # å¢åŠ  max_workers ä»¥åº”å¯¹å¯èƒ½ç¿»å€çš„ç§ç¾¤
    # ä½¿ç”¨ Context Manager ç®¡ç† ProcessPoolExecutor
    # [Performance] ä½¿ç”¨æ‰€æœ‰å¯ç”¨æ ¸å¿ƒ
    import multiprocessing
    # num_workers = multiprocessing.cpu_count()
    # å¦‚æœæ ¸å¿ƒæ•°è¿‡å¤šï¼Œé™åˆ¶ä¸€ä¸‹ä»¥å…å†…å­˜çˆ†ç‚¸ (e.g. 64æ ¸)
    # [Performance] æ¢å¤å¤šè¿›ç¨‹å¹¶è¡Œï¼Œå¼ºåˆ¶ä½¿ç”¨ fork æ¨¡å¼
    import multiprocessing
    
    # å°è¯•è·å– fork ä¸Šä¸‹æ–‡ (Linux/Unix é»˜è®¤ï¼Œä½†åœ¨æŸäº›é…ç½®ä¸‹å¯èƒ½è¢«è¦†ç›–)
    try:
        mp_context = multiprocessing.get_context("fork")
    except ValueError:
        # å¦‚æœä¸æ”¯æŒ fork (e.g. Windows)ï¼Œå›é€€åˆ°é»˜è®¤
        mp_context = None
        
    num_workers = multiprocessing.cpu_count()
    # é™åˆ¶æœ€å¤§ Worker æ•°
    # [Performance Tuning] çº¿ç¨‹æ•°å·²é™åˆ¶ä¸º 1ï¼Œç°åœ¨å¯ä»¥å…¨æ ¸è·‘äº†
    # num_workers = min(num_workers, 32) 
    um_workers = num_workers
    print(f"ğŸš€ Launching ProcessPoolExecutor with {num_workers} workers (Context: {mp_context})")
    
    # è¿™é‡Œçš„ if-else æ˜¯ä¸ºäº†ä¿ç•™ SyncExecutor ä½œä¸ºä¸€ä¸ª fallback é€‰é¡¹ï¼Œä½†æˆ‘ä»¬ç°åœ¨è¦åˆ‡å›å¹¶è¡Œ
    if True: 
        # Python 3.7+ æ”¯æŒ mp_context
        executor_cm = ProcessPoolExecutor(
            max_workers=num_workers, 
            initializer=worker_initializer, 
            initargs=initargs,
            mp_context=mp_context
        )
    else:
        # Fake Executor for debugging
        class SyncExecutor:
            def __enter__(self): 
                # Manually initialize worker state in main process
                print("ğŸ”§ Initializing Worker State in Main Process...")
                worker_initializer(*initargs)
                return self
            def __exit__(self, exc_type, exc_val, exc_tb): pass
            def map(self, func, iterable):
                return map(func, iterable)
            def shutdown(self, wait=True): pass
            
        executor_cm = SyncExecutor()
        print("âš ï¸ Running in SYNC mode (No Multiprocessing)")

    with executor_cm as shared_executor:
    
        # ä½¿ç”¨ OpenAI-ES (é˜²æ­¢å±€éƒ¨æœ€ä¼˜)
        opt_bypass = OpenAIESOptimizer(global_request_list, shared_executor, bypass=True, map_name=map_name, traffic_mid=traffic_mid, protocol=protocol, detector=detector, device=device)
        opt_nobypass = OpenAIESOptimizer(global_request_list, shared_executor, bypass=False, map_name=map_name, traffic_mid=traffic_mid, protocol=protocol, detector=detector, device=device)
        
        phase1_gens = 50
        phase2_gens = 100 
        
        # Phase 1
        print(f"\n=== Phase 1: Pre-training NoBypass ({phase1_gens} gens) ===")
        for gen in range(1, phase1_gens + 1):
            if not opt_nobypass.step(): break
            
            report = {
                "generation": gen,
                "bypass": opt_bypass.get_best_result(),
                "nobypass": opt_nobypass.get_best_result(),
                "status": "phase1_nobypass"
            }
            with open(json_filename, "w") as f: json.dump(report, f, indent=4)
        
        # Phase 2
        print(f"\n=== Phase 2: Transferring Knowledge & Training Bypass ===")
        opt_bypass.load_from_optimizer(opt_nobypass)
        
        max_it = 300
        total_gens = phase1_gens + phase2_gens
        
        final_status = "max_gens_reached"
        
        for gen in range(phase1_gens + 1, max_it + 1):
            if not opt_bypass.step(): break
            
            report = {
                "generation": gen,
                "bypass": opt_bypass.get_best_result(),
                "nobypass": opt_nobypass.get_best_result(),
                "status": "phase2_bypass"
            }
            with open(json_filename, "w") as f: json.dump(report, f, indent=4)
            
            # æ¯”è¾ƒé€»è¾‘
            p_bypass = opt_bypass.best_pure_power_found
            p_nobypass = opt_nobypass.best_pure_power_found
            s_bypass = opt_bypass.best_metrics.get('spec_occ', 0.0)
            s_nobypass = opt_nobypass.best_metrics.get('spec_occ', 0.0)

            # åœæ­¢æ¡ä»¶åˆ¤æ–­
            if p_bypass < float('inf') and p_nobypass < float('inf'):
                power_win = p_bypass < p_nobypass
                spec_tradeoff = s_bypass > s_nobypass
                if gen > total_gens:
                    if power_win and spec_tradeoff:
                        print(f"âœ… Bypass Wins with Trade-off! Stopping.")
                        final_status = "bypass_wins_tradeoff"
                        break
                    # å…¶ä»–æƒ…å†µç»§ç»­è·‘
        
        # è·å–æœ€ç»ˆç»“æœ
        result = {
            "config": {
                "topology": map_name,
                "protocol": protocol,
                "detector": detector,
                "traffic": traffic_mid
            },
            "bypass_result": opt_bypass.get_best_result(),
            "nobypass_result": opt_nobypass.get_best_result(),
            "status": final_status
        }
        
        opt_nobypass.close()
        opt_bypass.close()
        
    # Clean up temp file
    try: os.remove(json_filename)
    except: pass
        
    return result

if __name__ == "__main__":
    # é»˜è®¤å•æ¬¡è¿è¡Œ (ç”¨äºè°ƒè¯•)
    run_experiment("Tokyo", "BB84", "APD", "Low")
